#!/usr/bin/env python3
"""
Test Concurrent Implementation
Verify that the new concurrent scraper works and shows detailed URL logging
"""

import sys
import time
import requests
import json

def test_concurrent_implementation():
    """Test if the new concurrent implementation is working"""
    
    print("ğŸ§ª TESTING CONCURRENT IMPLEMENTATION")
    print("=" * 60)
    
    # Test URL (assumes Theodore is running on localhost:5002)
    base_url = "http://localhost:5002"
    
    # Test company - use a simple, fast website
    test_company = {
        "company_name": "Concurrent Test Company",
        "website": "https://httpbin.org"  # Simple, fast test website
    }
    
    print(f"ğŸ“‹ Test Company: {test_company['company_name']}")
    print(f"ğŸŒ Website: {test_company['website']}")
    print()
    
    # Submit company for processing
    print("ğŸ“¤ Submitting company for processing...")
    try:
        response = requests.post(
            f"{base_url}/api/process-company",
            json=test_company,
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            job_id = result.get('job_id')
            print(f"âœ… Request accepted, Job ID: {job_id}")
            
            if job_id:
                return monitor_concurrent_logging(base_url, job_id)
            else:
                print("âŒ No job ID returned")
                return False
        else:
            print(f"âŒ Request failed: {response.status_code}")
            print(f"Response: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ Request failed: {e}")
        return False

def monitor_concurrent_logging(base_url, job_id, max_wait=90):
    """Monitor the progress log for concurrent implementation features"""
    
    print(f"\\nâ³ Monitoring concurrent implementation for job {job_id}...")
    start_time = time.time()
    
    concurrent_features = []
    detailed_urls = []
    
    while time.time() - start_time < max_wait:
        try:
            response = requests.get(f"{base_url}/api/progress/{job_id}", timeout=5)
            
            if response.status_code == 200:
                progress_data = response.json().get('progress', {})
                processing_log = progress_data.get('processing_log', [])
                
                # Look for concurrent implementation features
                for log_entry in processing_log:
                    message = log_entry.get('message', '')
                    timestamp = log_entry.get('timestamp', '')
                    
                    # Check for detailed URL logging patterns
                    if any(pattern in message for pattern in [
                        'Analyzing robots.txt: http',
                        'Found sitemap: http',
                        'Analyzing default sitemap: http',
                        'Starting recursive crawling from: http',
                        'Crawling depth',
                        'Priority 1: http',
                        'Priority 2: http',
                        'Extracting: http',
                        'LLM prompt sent:',
                        'LLM response received in',
                        'AI analysis prompt sent:',
                        'AI analysis completed in'
                    ]):
                        if message not in [e['message'] for e in concurrent_features]:
                            concurrent_features.append({
                                'timestamp': timestamp,
                                'message': message
                            })
                            print(f"âœ… Concurrent: [{timestamp}] {message}")
                            
                            # Track detailed URLs specifically
                            if any(url_pattern in message for url_pattern in [
                                'robots.txt: http', 'sitemap: http', 'crawling from: http',
                                'Priority', 'Extracting: http'
                            ]):
                                detailed_urls.append(message)
                    
                    # Show phase completions
                    elif any(phase in message for phase in [
                        'Link Discovery complete', 'LLM Page Selection:',
                        'Content Extraction:', 'AI Content Analysis:', 'Research completed successfully'
                    ]):
                        print(f"ğŸ“Š Phase: [{timestamp}] {message}")
                
                # Check if job is completed
                status = progress_data.get('status', 'unknown')
                if status in ['completed', 'failed']:
                    break
                    
            else:
                print(f"   âš ï¸ Progress check failed: {response.status_code}")
                
        except Exception as e:
            print(f"   âš ï¸ Progress check error: {e}")
        
        time.sleep(2)  # Check every 2 seconds
    
    # Summary
    print(f"\\nğŸ“Š CONCURRENT IMPLEMENTATION RESULTS:")
    print(f"   Concurrent features found: {len(concurrent_features)}")
    print(f"   Detailed URLs logged: {len(detailed_urls)}")
    
    if len(concurrent_features) >= 5 and len(detailed_urls) >= 3:
        print(f"âœ… Concurrent implementation is working!")
        print(f"âœ… Detailed URL logging is functional!")
        print(f"ğŸ“‹ Sample concurrent features:")
        for feature in concurrent_features[:8]:
            print(f"   â€¢ [{feature['timestamp']}] {feature['message']}")
        return True
    elif len(concurrent_features) > 0:
        print(f"âš ï¸ Partial implementation detected")
        print(f"ğŸ“‹ Found features:")
        for feature in concurrent_features:
            print(f"   â€¢ [{feature['timestamp']}] {feature['message']}")
        return False
    else:
        print(f"âŒ No concurrent implementation features found")
        print(f"ğŸ’¡ The old implementation may still be running")
        return False

def main():
    """Main test execution"""
    
    success = test_concurrent_implementation()
    
    print("\\n" + "=" * 60)
    if success:
        print("ğŸ‰ CONCURRENT IMPLEMENTATION TEST: âœ… WORKING")
        print("âœ… Thread-safe LLM calls are functional")
        print("âœ… Detailed URL logging is working")
        print("âœ… Rate-limited concurrent approach is integrated")
    else:
        print("âŒ CONCURRENT IMPLEMENTATION TEST: âŒ NOT WORKING")
        print("âš ï¸ May need to restart Theodore to load new implementation")
        print("ğŸ’¡ Try: pkill -f 'python3 app.py' && python3 app.py")
    
    print("\\nğŸ“‹ Next Steps:")
    print("1. If working: Concurrent implementation successfully integrated! âœ…")
    print("2. If not working: Restart Theodore and test again")
    print("3. Check Theodore console output for worker pool initialization")

if __name__ == "__main__":
    main()