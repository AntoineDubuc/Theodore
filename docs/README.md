# Theodore - AI-Powered Company Intelligence System

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Quick Start](#quick-start)
- [Architecture](#architecture)
- [Development Setup](#development-setup)
- [Key Components](#key-components)
- [Usage Guide](#usage-guide)
- [Technical Decisions](#technical-decisions)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Overview

Theodore is an AI-powered company intelligence system designed to process large volumes of survey responses and extract comprehensive business insights. Originally built to help David process 400 survey responses (reducing manual research from 5-6 hours per 10-12 companies to automated processing), Theodore combines advanced web scraping, AI extraction, and vector search capabilities.

### Core Capabilities
- **AI-Powered Web Scraping**: Uses Crawl4AI with LLMExtractionStrategy for intelligent content extraction
- **Multi-Model AI Analysis**: Leverages AWS Bedrock (Nova Premier) and OpenAI for comprehensive analysis
- **Vector Search**: Pinecone integration for semantic search and company clustering
- **Structured Data Extraction**: Pydantic models ensure consistent data validation
- **Scalable Processing**: Designed for Lambda deployment and batch processing

### Key Technologies
- **Crawl4AI**: Advanced web scraping with AI-powered extraction
- **AWS Bedrock**: Enterprise AI models (Nova Premier, Titan Embeddings)
- **Pinecone**: Vector database for semantic search
- **OpenAI**: GPT models for extraction and analysis
- **Pydantic**: Data validation and serialization
- **Python AsyncIO**: High-performance concurrent processing

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- OpenAI API key
- AWS credentials (for Bedrock)
- Pinecone API key

### Installation
```bash
# Clone and setup
git clone <repository>
cd Theodore
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys
```

### Basic Usage
```python
from src.main_pipeline import TheodorePipeline
from src.models import CompanyIntelligenceConfig

# Initialize
config = CompanyIntelligenceConfig(max_companies=10)
pipeline = TheodorePipeline(config)

# Process companies
results = pipeline.process_survey_responses("data/survey.csv")
```

## ğŸ—ï¸ Architecture

Theodore follows a modular, pipeline-based architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Survey Data   â”‚ -> â”‚  Web Scraping    â”‚ -> â”‚  AI Extraction  â”‚
â”‚   (CSV Input)   â”‚    â”‚  (Crawl4AI)      â”‚    â”‚  (Bedrock/GPT)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         v                       v                       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Models    â”‚    â”‚  Vector Storage  â”‚    â”‚   Analysis &    â”‚
â”‚  (Pydantic)     â”‚    â”‚  (Pinecone)      â”‚    â”‚   Clustering    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

1. **Data Models** (`src/models.py`)
   - `CompanyData`: Main company information structure
   - `CompanyIntelligenceConfig`: System configuration
   - Pydantic validation for data integrity

2. **Web Scraping** (`src/crawl4ai_scraper.py`)
   - Multi-page crawling (homepage, about, services, etc.)
   - AI-powered content extraction using LLMExtractionStrategy
   - Intelligent data merging from multiple sources

3. **AI Analysis** (`src/bedrock_client.py`)
   - AWS Bedrock integration (Nova Premier model)
   - Structured analysis prompts
   - Embedding generation for vector storage

4. **Vector Storage** (`src/pinecone_client.py`)
   - Optimized metadata storage (5 key fields vs 62+ raw fields)
   - Semantic search capabilities
   - Efficient retrieval and clustering

5. **Main Pipeline** (`src/main_pipeline.py`)
   - Orchestrates the entire process
   - Handles batch processing and error recovery
   - Supports different processing modes

## ğŸ› ï¸ Development Setup

### Project Structure
```
Theodore/
â”œâ”€â”€ src/                    # Core application code
â”‚   â”œâ”€â”€ models.py          # Pydantic data models
â”‚   â”œâ”€â”€ crawl4ai_scraper.py # AI-powered web scraping
â”‚   â”œâ”€â”€ bedrock_client.py  # AWS Bedrock integration
â”‚   â”œâ”€â”€ pinecone_client.py # Vector database operations
â”‚   â””â”€â”€ main_pipeline.py   # Main orchestration
â”œâ”€â”€ data/                  # Input data and outputs
â”œâ”€â”€ tests/                 # Test suites
â”œâ”€â”€ scripts/               # Utility scripts
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ config/                # Configuration files
â””â”€â”€ requirements.txt       # Dependencies
```

### Environment Configuration
Create a `.env` file with:
```bash
# AI Service Keys
OPENAI_API_KEY=your_openai_key
AWS_ACCESS_KEY_ID=your_aws_key
AWS_SECRET_ACCESS_KEY=your_aws_secret
AWS_DEFAULT_REGION=us-east-1

# Vector Database
PINECONE_API_KEY=your_pinecone_key
PINECONE_INDEX_NAME=theodore-companies

# Optional: Custom endpoints
PINECONE_HOST=your_custom_host
```

### Development Commands
```bash
# Run tests
python -m pytest tests/

# Process single company (testing)
python tests/test_single_company.py

# Full pipeline (small batch)
python src/main_pipeline.py

# Check Pinecone index status
python scripts/pinecone_review.py

# Clear and reset data
python scripts/clear_pinecone.py
```

## ğŸ”§ Key Components Deep Dive

### 1. AI-Powered Web Scraping

Theodore's scraping system represents a significant evolution from basic web scraping to AI-powered extraction:

**Evolution Timeline:**
1. **Manual Regex** â†’ Basic pattern matching (limited accuracy)
2. **Crawl4AI Basic** â†’ Better content extraction but manual parsing
3. **LLMExtractionStrategy** â†’ AI-powered structured data extraction (current)

**Current Implementation:**
```python
# Schema-based extraction with Pydantic
class CompanyIntelligence(BaseModel):
    company_name: str = Field(description="Company name")
    industry: str = Field(description="Primary industry or sector")
    business_model: str = Field(description="Business model (B2B, B2C, SaaS, etc.)")
    # ... more fields

# AI extraction strategy
strategy = LLMExtractionStrategy(
    llm_config=LLMConfig(provider="openai/gpt-4o-mini", api_token=api_key),
    schema=CompanyIntelligence.model_json_schema(),
    extraction_type="schema"
)
```

**Key Fix: LLMConfig ForwardRef Issue**
- **Problem**: `TypeError: 'ForwardRef' object is not callable`
- **Solution**: Import `LLMConfig` from main `crawl4ai` module, not `crawl4ai.config`
- **Working Code**: `from crawl4ai import LLMConfig`

### 2. Optimized Vector Storage

**Metadata Optimization:**
- **Before**: 62+ fields stored as Pinecone metadata (inefficient, expensive)
- **After**: 5 essential fields in metadata, full data in separate storage
- **Essential Fields**: `company_name`, `industry`, `business_model`, `target_market`, `company_size`

```python
# Optimized approach
metadata = {
    "company_name": company.name,
    "industry": company.industry,
    "business_model": company.business_model,
    "target_market": company.target_market,
    "company_size": company.employee_count_range
}
```

### 3. Multi-Model AI Integration

**Primary Models:**
- **Extraction**: OpenAI GPT-4o-mini (fast, cost-effective for structured extraction)
- **Analysis**: AWS Bedrock Nova Premier (comprehensive business analysis)
- **Embeddings**: Amazon Titan Text Embeddings v2 (semantic search)

**Hybrid Approach Benefits:**
- OpenAI: Superior structured extraction capabilities
- Bedrock: Enterprise-grade analysis and AWS ecosystem integration
- Cost optimization through model selection

## ğŸ“ Usage Guide

### Processing Survey Data

1. **Prepare Input Data**
   - CSV format with company names and websites
   - Required columns: `name`, `website`
   - Optional: `survey_response`, `contact_info`

2. **Configure Processing**
   ```python
   config = CompanyIntelligenceConfig(
       max_companies=50,           # Batch size
       max_content_length=10000,   # Content truncation
       enable_clustering=True      # Sector analysis
   )
   ```

3. **Run Pipeline**
   ```python
   pipeline = TheodorePipeline(config)
   results = pipeline.process_csv("data/companies.csv")
   ```

4. **Access Results**
   - **Pinecone**: Semantic search and clustering
   - **Local Files**: Detailed company profiles
   - **Logs**: Processing status and errors

### Semantic Search Examples

```python
from src.pinecone_client import PineconeClient

client = PineconeClient(config)

# Find similar companies
results = client.semantic_search("SaaS B2B enterprise software", top_k=10)

# Industry clustering
fintech_companies = client.filter_by_metadata({"industry": "FinTech"})

# Complex queries
results = client.semantic_search(
    "AI machine learning startups",
    filter={"business_model": "B2B", "company_size": "startup"}
)
```

## ğŸ¯ Technical Decisions & Lessons Learned

### Major Technical Decisions

1. **Crawl4AI over Scrapy/BeautifulSoup**
   - **Reason**: Native AI extraction capabilities
   - **Benefit**: Structured data extraction without manual parsing
   - **Challenge**: LLMConfig ForwardRef issue (resolved)

2. **Hybrid AI Model Strategy**
   - **OpenAI**: Fast, accurate structured extraction
   - **AWS Bedrock**: Comprehensive analysis, enterprise features
   - **Result**: Optimal cost/performance balance

3. **Pinecone Metadata Optimization**
   - **Original**: 62+ fields in metadata (expensive, slow)
   - **Optimized**: 5 essential fields + separate full data storage
   - **Impact**: 90%+ cost reduction, improved performance

4. **AsyncIO for Concurrency**
   - **Reason**: I/O-bound operations (web scraping, AI calls)
   - **Implementation**: `asyncio` with rate limiting
   - **Result**: 5x+ performance improvement

### Key Lessons Learned

1. **AI Extraction Evolution**
   - Manual regex â†’ Basic scraping â†’ AI-powered extraction
   - Each step significantly improved data quality and reduced maintenance

2. **Vector Database Costs**
   - Metadata storage costs can quickly escalate
   - Strategic field selection crucial for production systems

3. **Error Handling & Recovery**
   - Web scraping requires robust error handling
   - Graceful degradation essential for batch processing

4. **Configuration Management**
   - Flexible configuration enables different use cases
   - Environment-based settings support dev/prod deployments

### Common Issues & Solutions

1. **LLMConfig ForwardRef Error**
   - **Fix**: `from crawl4ai import LLMConfig` (not from crawl4ai.config)

2. **Pinecone Dimension Mismatch**
   - **Fix**: Ensure embedding dimensions match index configuration

3. **Rate Limiting**
   - **Fix**: Implement delays between API calls, respect service limits

4. **Memory Usage**
   - **Fix**: Process data in batches, clear large objects after use

## ğŸ” Troubleshooting

### Common Error Patterns

**1. Crawl4AI Import Errors**
```bash
# Error: cannot import name 'LLMConfig'
# Fix: Use correct import
from crawl4ai import LLMConfig  # âœ… Correct
from crawl4ai.config import LLMConfig  # âŒ Wrong
```

**2. Pinecone Connection Issues**
```python
# Check index status
client.describe_index()

# Verify API key and host
import pinecone
pinecone.init(api_key="your_key")
```

**3. AWS Bedrock Access**
```bash
# Verify AWS credentials
aws sts get-caller-identity

# Check Bedrock model access
aws bedrock list-foundation-models --region us-east-1
```

### Development Tips

1. **Start Small**: Test with 1-2 companies before batch processing
2. **Monitor Costs**: Track API usage, especially for large batches
3. **Use Logs**: Comprehensive logging helps debug issues
4. **Incremental Development**: Test each component independently

### Performance Optimization

1. **Batch Processing**: Process companies in groups of 10-50
2. **Concurrent Requests**: Use asyncio with rate limiting
3. **Caching**: Cache successful extractions to avoid re-processing
4. **Resource Management**: Monitor memory and API quotas

---

## ğŸ“ Support

For questions or issues:
1. Check the troubleshooting section above
2. Review test files for usage examples
3. Examine log files for detailed error information
4. Test individual components in isolation

## ğŸ”„ Next Steps

Theodore is designed for extensibility. Consider these enhancements:
- Additional AI models (Anthropic Claude, etc.)
- Real-time processing capabilities
- Advanced clustering algorithms
- Dashboard/UI for non-technical users
- Integration with CRM systems

---

*This documentation reflects the current state of Theodore after resolving the Crawl4AI LLMExtractionStrategy integration and optimizing the vector storage approach.*